import os, json, time
import pandas as pd
from typing import List, Dict, Any
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from tqdm import tqdm
from openai import OpenAI

# ===== CONFIG =====
INPUT_CSV  = "reviews_clickcannabis_COM_TEXTO.csv"   # use o CSV filtrado
OUTPUT_CSV = "reviews_clickcannabis_ia.csv"
CHECKPOINT = "reviews_clickcannabis_ia.jsonl"
MODEL = "gpt-4o-mini"
BATCH_SIZE = 25          # ajuste se quiser
MAX_REVIEWS = None       # ex.: 300 para testar em amostra
TIMEOUT_S = 120

LABELS = [
 "Atendimento","√â Golpe?","Pre√ßo","Click",
 "Sono","Bem estar geral","Ansiedade","Dores","Alzheimer",
 "Enxaqueca","Autismo","TDAH","Fibromialgia","Epilepsia",
 "Tabagismo","Estresse","Produto: √ìleo","Produto: Gummy","Produto: T√≥pico"
]

LABEL_GUIDE = {
 "Atendimento": "Atendimento/suporte/funcion√°rios/p√≥s-venda.",
 "√â Golpe?": "Confiabilidade/seguran√ßa, men√ß√µes a golpe/fraude/scam.",
 "Pre√ßo": "Pre√ßo, caro/barato, custo-benef√≠cio, frete.",
 "Click": "Men√ß√µes √† Click Cannabis (marca/loja/site).",
 "Sono": "Dormir melhor/pior, ins√¥nia.",
 "Bem estar geral": "Bem-estar geral, qualidade de vida, relaxamento.",
 "Ansiedade": "Controle/redu√ß√£o da ansiedade, crises.",
 "Dores": "Dores e al√≠vio de dor n√£o espec√≠fico.",
 "Alzheimer": "Cita√ß√µes a Alzheimer/dem√™ncia.",
 "Enxaqueca": "Enxaqueca/cefaleia/migr√¢nea.",
 "Autismo": "Autismo/TEA.",
 "TDAH": "D√©ficit de aten√ß√£o/hiperatividade (TDAH).",
 "Fibromialgia": "Fibromialgia.",
 "Epilepsia": "Epilepsia/convuls√µes.",
 "Tabagismo": "Parar de fumar/cessa√ß√£o do tabaco.",
 "Estresse": "Estresse/tens√£o/sobrecarga.",
 "Produto: √ìleo": "√ìleo sublingual (‚Äò√≥leo‚Äô, ‚Äòoleo‚Äô, ‚Äòolio‚Äô).",
 "Produto: Gummy": "Gomas/balas/jelly/gummy.",
 "Produto: T√≥pico": "Pomada/creme/gel/t√≥pico na pele."
}

SYSTEM_PROMPT = f"""
Voc√™ √© um classificador multilabel de depoimentos PT-BR. Regras:
- Use apenas estes r√≥tulos: {LABELS}.
- Aplique 0..N r√≥tulos por depoimento, com bom senso (n√£o dependa s√≥ de palavras exatas).
- Retorne JSON v√°lido no formato pedido. D√™ uma justificativa curta (1‚Äì2 frases).
- Siga estas defini√ß√µes:
{json.dumps(LABEL_GUIDE, ensure_ascii=False, indent=2)}
"""

client = OpenAI(timeout=TIMEOUT_S)

def build_user_payload(items: List[Dict[str, Any]]) -> str:
    bloco = []
    for it in items:
        t = (it.get("texto") or "").strip()
        bloco.append({"id": it["idx"], "texto": t[:4000]})
    task = {
        "tarefa": "Classificar multilabel cada depoimento nos r√≥tulos predefinidos.",
        "rotulos": LABELS,
        "formato_resposta": {
            "type": "object",
            "properties": {
                "itens": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {"type":"integer"},
                            "categorias": {"type":"array","items":{"type":"string"}},
                            "confianca": {"type":"number"},
                            "justificativa": {"type":"string"}
                        },
                        "required": ["id","categorias","confianca","justificativa"]
                    }
                }
            },
            "required": ["itens"]
        },
        "itens": bloco
    }
    return json.dumps(task, ensure_ascii=False)

@retry(reraise=True, stop=stop_after_attempt(5),
       wait=wait_exponential(min=2, max=20),
       retry=retry_if_exception_type(Exception))
def classify_batch(batch_rows: List[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:
    user_payload = build_user_payload(batch_rows)
    resp = client.chat.completions.create(
        model=MODEL, temperature=0.0,
        response_format={"type": "json_object"},
        messages=[{"role":"system","content": SYSTEM_PROMPT},
                  {"role":"user","content": user_payload}]
    )
    data = json.loads(resp.choices[0].message.content)
    out = {}
    for item in data.get("itens", []):
        idx = int(item["id"])
        out[idx] = {
            "categorias": [c for c in item.get("categorias", []) if c in LABELS],
            "confianca": float(item.get("confianca", 0.0)),
            "justificativa": item.get("justificativa", "")
        }
    return out

def main():
    if not os.getenv("OPENAI_API_KEY"):
        raise SystemExit("Defina OPENAI_API_KEY (export OPENAI_API_KEY='...').")

    if not os.path.exists(INPUT_CSV):
        raise SystemExit(f"Arquivo n√£o encontrado: {INPUT_CSV}")

    base = pd.read_csv(INPUT_CSV)
    if "texto" not in base.columns:
        raise SystemExit("CSV precisa ter coluna 'texto'.")

    # montar lista a classificar
    rows = [{"idx": i, "texto": row["texto"]} for i, row in base.iterrows()]
    if MAX_REVIEWS is not None:
        rows = rows[:MAX_REVIEWS]

    # se j√° existe OUTPUT_CSV, retoma sem duplicar
    done_keys = set()
    if os.path.exists(OUTPUT_CSV):
        done = pd.read_csv(OUTPUT_CSV)
        done_keys = set(done.index.tolist())

    results_map = {}
    for start in tqdm(range(0, len(rows), BATCH_SIZE), desc="Classificando"):
        batch = rows[start:start+BATCH_SIZE]
        # pular itens j√° existentes (retomada simples)
        batch = [b for b in batch if b["idx"] not in done_keys]
        if not batch:
            continue

        out = classify_batch(batch)

        # checkpoint bruto
        with open(CHECKPOINT, "a", encoding="utf-8") as f:
            for it in batch:
                idx = it["idx"]
                res = out.get(idx, {"categorias":[],"confianca":0.0,"justificativa":""})
                f.write(json.dumps({"idx": idx, **res}, ensure_ascii=False) + "\n")

        # salvar/atualizar CSV incrementalmente
        part = []
        for it in batch:
            idx = it["idx"]
            row = base.loc[idx].to_dict()
            res = out.get(idx, {"categorias":[],"confianca":0.0,"justificativa":""})
            row["categorias_ia"] = ", ".join(res["categorias"])
            row["confianca_ia"] = res["confianca"]
            row["justificativa_ia"] = res["justificativa"]
            part.append(row)
        part_df = pd.DataFrame(part)

        if os.path.exists(OUTPUT_CSV):
            existing = pd.read_csv(OUTPUT_CSV)
            merged = pd.concat([existing, part_df], ignore_index=True)
            merged = merged.drop_duplicates(subset=["review_id","texto"], keep="last") \
                     if "review_id" in merged.columns \
                     else merged.drop_duplicates(subset=["autor_nome","data_original","texto"], keep="last")
            merged.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
        else:
            part_df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

        time.sleep(0.5)

    print(f"\n‚úÖ Classifica√ß√£o conclu√≠da. Arquivo salvo: {OUTPUT_CSV}")
    # resumo r√°pido
    final = pd.read_csv(OUTPUT_CSV)
    counts = {}
    for lab in LABELS:
        counts[lab] = int(final["categorias_ia"].fillna("").str.contains(rf"\b{lab}\b", regex=True).sum())
    pd.DataFrame(sorted(counts.items(), key=lambda x:(-x[1], x[0])), columns=["tema","qtd"]) \
      .to_csv("resumo_categorias_ia.csv", index=False, encoding="utf-8-sig")
    print("üìä Resumo salvo em: resumo_categorias_ia.csv")

if __name__ == "__main__":
    main()
